{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anatomy of a Python Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few times that I tried to learn Python, it felt like learning a bunch of made-up rules about an imaginary universe. It turns out that Python is kind of like an imaginary universe with made-up rules. That's part of what makes Python and programming languages so much fun.\n",
    "\n",
    "But it can also make learning Python difficult if you don't really know what the imaginary universe looks like, or how it functions, or how it relates to your universe and your specific goals — such as doing text analysis or making a Twitter bot or creating a network visualization.\n",
    "\n",
    "In this lesson, we're going to demonstrate what Python looks like in action, so you can get a feel for its structure and flow. Don't get too bogged down in the details for now. Just try to get a sense — at an abstract level — of how Python works and how you might use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.pixabay.com/photo/2017/01/31/23/21/animal-2028134_960_720.png\" alt=\"The command line\" width=\"100%\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a chunk of Python code. These lines, when put together, do something simple yet important. They count and display the most frequent words in a text file. The example below specifically counts and displays the 40 most frequent words in Charlotte Perkins Gilman's short story \"The Yellow Wallpaper\" (1892)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('john', 45),\n",
       " ('one', 33),\n",
       " ('said', 30),\n",
       " ('would', 27),\n",
       " ('get', 24),\n",
       " ('see', 24),\n",
       " ('room', 24),\n",
       " ('pattern', 24),\n",
       " ('paper', 23),\n",
       " ('like', 21),\n",
       " ('little', 20),\n",
       " ('much', 16),\n",
       " ('good', 16),\n",
       " ('think', 16),\n",
       " ('well', 15),\n",
       " ('know', 15),\n",
       " ('go', 15),\n",
       " ('really', 14),\n",
       " ('thing', 14),\n",
       " ('wallpaper', 13),\n",
       " ('night', 13),\n",
       " ('long', 12),\n",
       " ('course', 12),\n",
       " ('things', 12),\n",
       " ('take', 12),\n",
       " ('always', 12),\n",
       " ('could', 12),\n",
       " ('jennie', 12),\n",
       " ('great', 11),\n",
       " ('says', 11),\n",
       " ('feel', 11),\n",
       " ('even', 11),\n",
       " ('used', 11),\n",
       " ('dear', 11),\n",
       " ('time', 11),\n",
       " ('enough', 11),\n",
       " ('away', 11),\n",
       " ('want', 11),\n",
       " ('never', 10),\n",
       " ('must', 10)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def split_into_words(any_chunk_of_text):\n",
    "    lowercase_text = any_chunk_of_text.lower()\n",
    "    split_words = re.split(\"\\W+\", lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper_Charlotte-Perkins-Gilman.txt\"\n",
    "number_of_desired_words = 40\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']\n",
    "\n",
    "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
    "\n",
    "all_the_words = split_into_words(full_text)\n",
    "meaningful_words = [word for word in all_the_words if word not in stopwords]\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
    "\n",
    "most_frequent_meaningful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating word frequency is a very basic form of computational text analysis. Typically, it's not terribly interesting on its own, especially with a single short text. But calculating word frequency *is* important, and it's at the center of most text analysis approaches, even far more complicated ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Python Review\n",
    ":class: pythonreview\n",
    "It's important to emphasize that the code above is just *one* way to count words in a text file with Python. This is not the one right way. There is no right way to count words in a text file or to do anything else in Python.\n",
    "\n",
    "Rather than asking \"Is this code *right*?\", you want to ask yourself:\n",
    "- \"Is this code efficient?\"\n",
    "- \"Is this code readable?\"\n",
    "- \"Does this code help me accomplish my goal?\"\n",
    "\n",
    "Sometimes you'll prioritize one of these concerns over another. Maybe your code isn't as efficient as humanly possible, but if it gets the job done, and you understand it, then you might not care about maximum efficiency. Our main goal for this class is to study and make arguments about culture, not (necessarily) to become the most efficient software developers.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Anatomy of a Python Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries/Packages/Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready for some great Python news? You don't have to code everything by yourself from scratch! Many other people have written Python code that you can `import` into your own code, which will save you time and do a lot of work behind-the-scenes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the code written and packaged up by other people a \"library,\" \"package,\" or \"module.\" We'll talk more about them [in a later lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/More-Lists-Loops.html##Reading-in-a-CSV-File). For now simply know that you `import` libraries/packages/modules at the very top of a Python script for later use.\n",
    "\n",
    "- [`Counter`](https://stackabuse.com/introduction-to-pythons-collections-module/) will help me count words\n",
    "- [`re`](https://docs.python.org/3/library/re.html#regular-expression-syntax), short for regular expressions, is basically a fancy find-and-replace that will help me split \"The Yellow Wallpaper\" into individual words and get rid of trailing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `import`ing modules and libraries, you typically `def`ine your \"functions.\" Functions are a nifty way to bundle up code so that you can use them again later. Functions also keep your code neat and tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_words(any_chunk_of_text):\n",
    "    words = re.split(\"\\W+\", any_chunk_of_text.lower())\n",
    "    return words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're making a function called `split_into_words`, which takes in any chunk of text, transforms that text to lower-case, and splits the text into a list of clean words without punctuation or spaces. We're not actually using the function yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Filepaths and [Assign Variables](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Python/Variables.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we establish some variables that we're going to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper_Charlotte-Perkins-Gilman.txt\"\n",
    "number_of_desired_words = 40\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the filepath of the short story in order to read it, so we make a variable called `filepath_of_text`. We also make a variable called `number_of_desired_words`, which will eventually tell the script how many words to display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we also make a variable called `stopwords` and plug in a list of common English language \"stop words\"—that is, a list of some of the most frequently occurring English language words. Stop words are typically removed from a text before computational analysis in order to shift the focus to less frequently occurring, more \"meaningful\" words.\n",
    "\n",
    "```{margin} Food for Thought\n",
    "What are the consequences of excluding stopwords from our analysis? When might we want to include such words in an analysis?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = open(filepath_of_text, encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line above opens Charlotte Perkins Gilman's \"The Yellow Wallpaper,\" reads in the novel, and assigns it to the variable `full_text`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate and Analyze File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count the words in \"The Yellow Wallpaper,\" we need to break the full text into individual words. Below  we call the function `split_into_words`, which we created earlier, and use it to split the `full_text` of the story into individual words. Then we assign this value to the variable `all_the_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_words = split_into_words(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we remove stopwords from our list. The line of code below makes a new list of words that includes every word in `all_the_words` if it does *not* appear in `stopwords` (aka it nixes the stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_words = [word for word in all_the_words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to count! We plug `meaningful_words` into our `Counter`, which gives us a tally of how many times each word in the story appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful_words_tally = Counter(meaningful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we pull out the top 40 most frequently occurring words from our complete tally. We make one final variable and grab our top `number_of_desired_words`, which we previously established as `40`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display our results by running a cell with the variable name `most_frequent_meaningful_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('john', 45),\n",
       " ('one', 33),\n",
       " ('said', 30),\n",
       " ('would', 27),\n",
       " ('get', 24),\n",
       " ('see', 24),\n",
       " ('room', 24),\n",
       " ('pattern', 24),\n",
       " ('paper', 23),\n",
       " ('like', 21),\n",
       " ('little', 20),\n",
       " ('much', 16),\n",
       " ('good', 16),\n",
       " ('think', 16),\n",
       " ('well', 15),\n",
       " ('know', 15),\n",
       " ('go', 15),\n",
       " ('really', 14),\n",
       " ('thing', 14),\n",
       " ('wallpaper', 13),\n",
       " ('night', 13),\n",
       " ('long', 12),\n",
       " ('course', 12),\n",
       " ('things', 12),\n",
       " ('take', 12),\n",
       " ('always', 12),\n",
       " ('could', 12),\n",
       " ('jennie', 12),\n",
       " ('great', 11),\n",
       " ('says', 11),\n",
       " ('feel', 11),\n",
       " ('even', 11),\n",
       " ('used', 11),\n",
       " ('dear', 11),\n",
       " ('time', 11),\n",
       " ('enough', 11),\n",
       " ('away', 11),\n",
       " ('want', 11),\n",
       " ('never', 10),\n",
       " ('must', 10)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_meaningful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can display our results by `print`ing the variable `most_frequent_meaningful_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('john', 45), ('one', 33), ('said', 30), ('would', 27), ('get', 24), ('see', 24), ('room', 24), ('pattern', 24), ('paper', 23), ('like', 21), ('little', 20), ('much', 16), ('good', 16), ('think', 16), ('well', 15), ('know', 15), ('go', 15), ('really', 14), ('thing', 14), ('wallpaper', 13), ('night', 13), ('long', 12), ('course', 12), ('things', 12), ('take', 12), ('always', 12), ('could', 12), ('jennie', 12), ('great', 11), ('says', 11), ('feel', 11), ('even', 11), ('used', 11), ('dear', 11), ('time', 11), ('enough', 11), ('away', 11), ('want', 11), ('never', 10), ('must', 10)]\n"
     ]
    }
   ],
   "source": [
    "print(most_frequent_meaningful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines that begin with a hash symbol `#` are ignored from the execution of the code. You can thus use a hash symbol `#` to insert human language comments directly into the code — notes or instructions to yourself and others.\n",
    "\n",
    "In some cases, you might want to write a long comment. To insert a multi-line comment, you can insert the comment between three quotations marks `\"\"\" \"\"\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('john', 45),\n",
       " ('one', 33),\n",
       " ('said', 30),\n",
       " ('would', 27),\n",
       " ('get', 24),\n",
       " ('see', 24),\n",
       " ('room', 24),\n",
       " ('pattern', 24),\n",
       " ('paper', 23),\n",
       " ('like', 21),\n",
       " ('little', 20),\n",
       " ('much', 16),\n",
       " ('good', 16),\n",
       " ('think', 16),\n",
       " ('well', 15),\n",
       " ('know', 15),\n",
       " ('go', 15),\n",
       " ('really', 14),\n",
       " ('thing', 14),\n",
       " ('wallpaper', 13),\n",
       " ('night', 13),\n",
       " ('long', 12),\n",
       " ('course', 12),\n",
       " ('things', 12),\n",
       " ('take', 12),\n",
       " ('always', 12),\n",
       " ('could', 12),\n",
       " ('jennie', 12),\n",
       " ('great', 11),\n",
       " ('says', 11),\n",
       " ('feel', 11),\n",
       " ('even', 11),\n",
       " ('used', 11),\n",
       " ('dear', 11),\n",
       " ('time', 11),\n",
       " ('enough', 11),\n",
       " ('away', 11),\n",
       " ('want', 11),\n",
       " ('never', 10),\n",
       " ('must', 10)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example Python code for\n",
    "calculating word frequency\n",
    "in a text file\n",
    "\"\"\"\n",
    "\n",
    "# Import Libraries and Modules\n",
    "\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Define Functions\n",
    "\n",
    "def split_into_words(any_chunk_of_text):\n",
    "    lowercase_text = any_chunk_of_text.lower()\n",
    "    split_words = re.split(\"\\W+\", lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "# Define Filepaths and Assign Variables\n",
    "\n",
    "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper_Charlotte-Perkins-Gilman.txt\"\n",
    "number_of_desired_words = 40\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp']\n",
    "\n",
    "# Read in File\n",
    "\n",
    "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
    "\n",
    "# Manipulate and Analyze File\n",
    "\n",
    "all_the_words = split_into_words(full_text)\n",
    "meaningful_words = [word for word in all_the_words if word not in stopwords]\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
    "\n",
    "# Output Results\n",
    "\n",
    "most_frequent_meaningful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Life of a Python Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook / JupyterLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary way that we're going to write and run Python in this class is through JupyterLab and Jupyter notebooks. As we've already covered, Jupyter notebooks are documents that can combine live code, explanatory text, and nice displays of data, which makes them great for teaching and learning.\n",
    "\n",
    "But it's also a fully functional way to run Python. By running a cell of Python code in a Jupyter notebook, you can:\n",
    "- read files from your computer and write files to your computer\n",
    "- make and save a bar chart\n",
    "- collect data from YouTube or Spotify\n",
    "- programmatically tweet from a Twitter bot account\n",
    "- and a lot more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, by adding two lines of code at the bottom of our script, I can output the most frequent words from \"The Yellow Wallpaper\" into a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{code-block} python\n",
    ":emphasize-lines: 16, 17\n",
    "def split_into_words(any_chunk_of_text):\n",
    "    lowercase_text = any_chunk_of_text.lower()\n",
    "    split_words = re.split(\"\\W+\", lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "filepath_of_text = \"../texts/literature/The-Yellow-Wallpaper.txt\"\n",
    "nltk_stop_words = stopwords.words(\"english\")\n",
    "number_of_desired_words = 40\n",
    "\n",
    "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
    "\n",
    "all_the_words = split_into_words(full_text)\n",
    "meaningful_words = [word for word in all_the_words if word not in nltk_stop_words]\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
    "\n",
    "with open(\"most-frequent-words-Yellow-Wallpaper.txt\", \"w\") as file_object:\n",
    "    file_object.write(str(most_frequent_meaningful_words))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Editor —> Command Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run a Python script by writing it in a text editor and then running it from the command line.\n",
    "\n",
    "If you copy and paste the code above into a simple text editor (like TextEdit or NotePad) and name the file with the extension \".py\" (the file extension for Python code), you should be able to run the script from your command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Python-plain-text.png\"  >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All you need to do is call python with the name of the Python file (and make sure that the script includes the correct file path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "command_line"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('john', 45), ('one', 33), ('said', 30), ('would', 27), ('get', 24), ('see', 24), ('room', 24), ('pattern', 24), ('paper', 23), ('like', 21), ('little', 20), ('much', 16), ('good', 16), ('think', 16), ('well', 15), ('know', 15), ('go', 15), ('really', 14), ('thing', 14), ('wallpaper', 13), ('night', 13), ('long', 12), ('course', 12), ('things', 12), ('take', 12), ('always', 12), ('could', 12), ('jennie', 12), ('great', 11), ('says', 11), ('feel', 11), ('even', 11), ('used', 11), ('dear', 11), ('time', 11), ('enough', 11), ('away', 11), ('want', 11), ('never', 10), ('must', 10)]\n"
     ]
    }
   ],
   "source": [
    "!python word_frequency_Yellow_Wallpaper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it's possible to write Python from TextEdit, it's not very common, because it's a pain. It's much more common to write Python code in a text editor like Atom, as shown below. You can see that there's all sorts of formatting and functionality that makes the code writing faster and easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/Python-Atom.png\"  >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write Python scripts such that they can work with different files or any file you want it to. With a few small alterations, our word frequency script can crunch numbers for Grimms Fairy Tales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "command_line"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('said', 911), ('little', 498), ('one', 454), ('king', 438), ('went', 385), ('came', 359), ('go', 266), ('away', 239), ('old', 233), ('man', 225), ('good', 211), ('took', 210), ('two', 209), ('woman', 199), ('saw', 193), ('could', 193), ('come', 186), ('time', 184), ('day', 180), ('would', 178), ('well', 177), ('_', 163), ('home', 163), ('back', 161), ('shall', 159), ('eyes', 157), ('three', 153), ('daughter', 150), ('mother', 145), ('house', 142), ('thought', 139), ('must', 139), ('forest', 138), ('great', 136), ('cried', 134), ('take', 134), ('long', 133), ('door', 132), ('nothing', 130), ('let', 130)]\n"
     ]
    }
   ],
   "source": [
    "!python word_frequency.py ../texts/literature/Grimms-Fairy-Tales.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or Louisa May Alcott's *Little Women*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "command_line"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('jo', 1395), ('one', 898), ('said', 839), ('little', 758), ('meg', 697), ('amy', 659), ('laurie', 608), ('like', 604), ('beth', 482), ('good', 479), ('would', 440), ('see', 422), ('m', 407), ('go', 400), ('old', 394), ('mother', 382), ('never', 375), ('much', 375), ('well', 371), ('could', 359), ('away', 340), ('time', 330), ('mr', 324), ('know', 321), ('march', 320), ('made', 303), ('home', 286), ('young', 285), ('girls', 281), ('think', 280), ('day', 279), ('come', 278), ('say', 277), ('went', 274), ('came', 273), ('dear', 265), ('got', 260), ('face', 260), ('make', 257), ('asked', 255)]\n"
     ]
    }
   ],
   "source": [
    "!python word_frequency.py ../texts/literature/Little-Women_Louisa-May-Alcott.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or any other text your heart desires!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
