{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "## HW 8 \u2014 Term Frequency\u2013Inverse Document Frequency"}, {"cell_type": "markdown", "metadata": {}, "source": "[Download relevant files here](https://melaniewalsh.org/TF-IDF.zip)"}, {"cell_type": "markdown", "metadata": {}, "source": "**HW Instructions** Download this Jupyter notebook, work through it, and run all the cells. Then answer the 3 questions at the end. When you're finished, save this file as \"Your-Last-Name-HW-8.ipynb\" and submit it to Canvas by Friday, April 9th at 5pm.\n\n*This notebook is exactly the same as [the TF-IDF lesson](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/TF-IDF.html), and submitting either notebook is fine."}, {"cell_type": "markdown", "metadata": {}, "source": "In this lesson, we're going to learn about a text analysis method called **term frequency\u2013inverse document frequency** (tf\u2013idf). This method will help us identify the most unique words in a document from a given corpus. "}, {"cell_type": "markdown", "metadata": {}, "source": ">term = word <br>\n>document = text (or chunk of a text) <br>\n>corpus = collection of texts <br>"}, {"cell_type": "markdown", "metadata": {}, "source": "## Why is tf\u2013idf Useful?\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## The Basic Math"}, {"cell_type": "markdown", "metadata": {}, "source": "> `term_frequency * inverse_document_frequency`"}, {"cell_type": "markdown", "metadata": {}, "source": "## Breaking Down the Formula\n"}, {"cell_type": "markdown", "metadata": {}, "source": "> `term_frequency = number of times a given word appears in story or text`"}, {"cell_type": "markdown", "metadata": {}, "source": "`inverse_document_frequency` equals the total number of short stories  divided by the number of short stories that contain the given word...\n\n> `total_number_of_documents / number_of_documents_with_term`\n\n...the result of which we're going to take the logarithm of and then add 1\n\n> `inverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1`\n\nDo you see how if we flipped the fraction \u2014 making it `number_of_documents_with_term /  total_number_of_documents`\u2014 that would just be \"document frequency\"? By inverting this fraction, however, we get \"inverse document frequency.\""}, {"cell_type": "markdown", "metadata": {}, "source": "## The Formula in Action"}, {"cell_type": "markdown", "metadata": {}, "source": "**\"said\" vs \"pigeons\"**"}, {"cell_type": "markdown", "metadata": {}, "source": "Using this formula, we're going to calculate and compare the tf\u2013idf scores for the word \"said\" and the word \"pigeons\" in \"The Girl Who Raised Pigeons,\" the first short story in *Lost in the City*."}, {"cell_type": "markdown", "metadata": {}, "source": "We need the log() function for our calculation, so we're going to import it from the `math` package."}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "from math import log"}, {"cell_type": "markdown", "metadata": {}, "source": "**\"said\"**"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "total_number_of_documents = 14 #total number of short stories in *Lost in the City*\nnumber_of_documents_with_term = 13 #number of short stories the contain the word \"said\""}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "term_frequency = 47 #number of times \"said\" appears in \"The Girl Who Raised Pigeons\"\ninverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "term_frequency * inverse_document_frequency"}, {"cell_type": "markdown", "metadata": {}, "source": "**\"pigeons\"**"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "total_number_of_documents = 14 #total number of short stories in *Lost in the City*\nnumber_of_documents_with_term = 2 #number of short stories the contain the word \"pigeons\""}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "term_frequency = 30 #number of times \"pigeons\" appears in \"The Girl Who Raised Pigeons\"\ninverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "term_frequency * inverse_document_frequency"}, {"cell_type": "markdown", "metadata": {}, "source": "**tf\u2013idf Scores**\n\n\"said\" = 50.48<br>\n\"pigeons\" = 88.38"}, {"cell_type": "markdown", "metadata": {}, "source": "Though the word \"said\" appears 47 times in \"The Girl Who Raised Pigeons\" and the word \"pigeons\" only appears 30 times, \"pigeons\" has a higher tf\u2013idf score than \"said\" because it's a rarer word. The word \"pigeons\" appears in 2 of 14 stories, while \"said\" appears in 13 of 14 stories, almost all of them."}, {"cell_type": "markdown", "metadata": {}, "source": "## tf\u2013idf with scikit-learn"}, {"cell_type": "markdown", "metadata": {}, "source": "## Import Libraries"}, {"cell_type": "markdown", "metadata": {}, "source": "We could continue calculating tf\u2013idf scores in this manner \u2014 by doing all the math with Python \u2014 but conveniently there's a Python library that can calculate tf\u2013idf scores in just a few lines of code.\n\nThis library is called [scikit-learn](https://scikit-learn.org/stable/index.html), imported as `sklearn`. It's a popular Python library for machine learning approaches such as clustering, classification, and regression, among others. Though we're not doing any machine learning in this lesson, we're nevertheless going to use scikit-learn's `TfidfVectorizer` and `CountVectorizer`."}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Requirement already satisfied: sklearn in /Users/melaniewalsh/anaconda3/lib/python3.7/site-packages (0.0)\n", "Requirement already satisfied: scikit-learn in /Users/melaniewalsh/anaconda3/lib/python3.7/site-packages (from sklearn) (0.20.3)\n", "Requirement already satisfied: scipy>=0.13.3 in /Users/melaniewalsh/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.3.1)\n", "Requirement already satisfied: numpy>=1.8.2 in /Users/melaniewalsh/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.17.4)\n"]}], "source": "!pip install sklearn"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer"}, {"cell_type": "markdown", "metadata": {}, "source": "We're also going to import `pandas` and change two of its default display settings. We're going to increase the maximum number of rows that pandas will display, and we're going to format numbers in a special way. If it's a decimal number, format to three decimal places; if it's a whole number, round to the whole number."}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": "import pandas as pd\npd.set_option(\"max_rows\", 200)\npd.set_option(\"max_columns\", 200)\npd.options.display.float_format = lambda value : '{:.0f}'.format(value) if round(value,0) == value else '{:,.3f}'.format(value)"}, {"cell_type": "markdown", "metadata": {}, "source": "Finally, we're going to import two libraries that will help us work with files and the file system: [`pathlib`](https://docs.python.org/3/library/pathlib.html#basic-use) and [`glob`](https://docs.python.org/3/library/glob.html). These libraries will help us read in all the short story text files from *Lost in the City*."}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "from pathlib import Path  \nimport glob"}, {"cell_type": "markdown", "metadata": {}, "source": "## Set Directory Path"}, {"cell_type": "markdown", "metadata": {}, "source": "Below we're setting the directory filepath that contains all the short story text files that we want to analyze."}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": "directory_path = \"../texts/literature/Lost-in-the-City_Stories/\""}, {"cell_type": "markdown", "metadata": {}, "source": "Then we're going to use `glob` and `Path` to make a list of all the short story filepaths in that directory and a list of all the short story titles."}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "text_files = glob.glob(f\"{directory_path}/*.txt\")\ntext_titles = [Path(text).stem for text in text_files]"}, {"cell_type": "markdown", "metadata": {}, "source": "Let's display them to make sure they're correct:"}, {"cell_type": "code", "execution_count": 19, "metadata": {"scrolled": true, "tags": ["hide_output"]}, "outputs": [{"data": {"text/plain": ["(['../texts/literature/Lost-in-the-City_Stories/11-Gospel.txt',\n", "  '../texts/literature/Lost-in-the-City_Stories/13-A-Dark-Night.txt',\n", "  '../texts/literature/Lost-in-the-City_Stories/01-The-Girl-Who-Raised-Pigeons.txt',\n", "  '../texts/literature/Lost-in-the-City_Stories/12-A-New-Man.txt',\n", "  '../texts/literature/Lost-in-the-City_Stories/02-The-First-Day.txt',\n", "  '../texts/literature/Lost-in-the-City_Stories/07-The-Sunday-Following-Mother\u2019S-Day.txt',\n", "  '../texts/literature/Lost-in-the-City_Stories/03-The-Night-Rhonda-Ferguson-Was-Killed.txt',\n", "  '../texts/literature/Lost-in-the-City_Stories/05-The-Store.txt',\n", "  '../texts/literature/Lost-in-the-City_Stories/08-Lost-In-The-City.txt',\n", "  '../texts/literature/Lost-in-the-City_Stories/14-Marie.txt',\n", "  '../texts/literature/Lost-in-the-City_Stories/09-His-Mother\u2019S-House.txt',\n", "  '../texts/literature/Lost-in-the-City_Stories/10-A-Butterfly-On-F-Street.txt',\n", "  '../texts/literature/Lost-in-the-City_Stories/06-An-Orange-Line-Train-To-Ballston.txt',\n", "  '../texts/literature/Lost-in-the-City_Stories/04-Young-Lions.txt'],\n", " ['11-Gospel',\n", "  '13-A-Dark-Night',\n", "  '01-The-Girl-Who-Raised-Pigeons',\n", "  '12-A-New-Man',\n", "  '02-The-First-Day',\n", "  '07-The-Sunday-Following-Mother\u2019S-Day',\n", "  '03-The-Night-Rhonda-Ferguson-Was-Killed',\n", "  '05-The-Store',\n", "  '08-Lost-In-The-City',\n", "  '14-Marie',\n", "  '09-His-Mother\u2019S-House',\n", "  '10-A-Butterfly-On-F-Street',\n", "  '06-An-Orange-Line-Train-To-Ballston',\n", "  '04-Young-Lions'])"]}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": "text_files, text_titles"}, {"cell_type": "markdown", "metadata": {}, "source": "## Calculate Word Frequency (Optional Step)"}, {"cell_type": "markdown", "metadata": {}, "source": "This is an optional step, but for the sake of comparison, we're first going to calculate the raw frequency for every word in every story with scikit-learn's [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Later, when we calculate our tf\u2013idf scores, we can compare these two methods and see how tf\u2013idf helps us find more unique words.\n\n(Machine learning approaches require that you transform words into a \"vector,\" aka a series of numbers. This is what `CountVectorizer` does. But it's also just a convenient way to tokenize and count words.)"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": "#Initialize CountVectorizer with desired parameters\ncount_vectorizer= CountVectorizer(input='filename', stop_words='english')\n\n#Plug in \"text_files,\" which contains all our short stories, to the initialized count_vectorizer\nword_count_vector = count_vectorizer.fit_transform(text_files)"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": "#Make a DataFrame out of the word count vector and sort by title\nword_count_df = pd.DataFrame(word_count_vector.toarray(), index=text_titles, columns=count_vectorizer.get_feature_names())\nword_count_df = word_count_df.sort_index()\n\n#Add column for number of times each word appears in all the documents\nword_count_df.loc['Document Frequency'] = (word_count_df > 0).sum()"}, {"cell_type": "markdown", "metadata": {}, "source": "This dataframe `word_count_df` displays all the words that appear in *Lost in the City*, how many times each word appears in each story, and how many times each word appears at least once across all the stories (the very last row of numbers titled \"Document Frequency\")."}, {"cell_type": "markdown", "metadata": {}, "source": "Let's look at a sample of 10 words. You can run the cell again to look at a different sample of words."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "word_count_df.sample(10, axis='columns')"}, {"cell_type": "markdown", "metadata": {}, "source": "Let's zoom in on some specific words."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "word_count_df[['pigeons', 'school', 'said', 'gospelteers', 'church', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]"}, {"cell_type": "markdown", "metadata": {}, "source": "To find the top 10 most frequent words in every story, we're going to make and run the following function: `get_top_n_counts()`"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [], "source": "def get_top_n_counts(dataframe, top_n=10):\n    pretty_df = dataframe.stack().groupby(level=0).nlargest(top_n).reset_index()\n    pretty_df = pretty_df.rename(columns={0:'count', 'level_1': 'story', 'level_2': 'word'})\n    pretty_df = pretty_df.drop(columns='level_0')\n    pretty_df['word_freq_rank'] = pretty_df.groupby('story')['count'].rank(method='min', ascending=False)\n    return pretty_df"}, {"cell_type": "markdown", "metadata": {}, "source": "This function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 most frequent words in every story. Finally, it will produce a dataframe with a new column `word_freq_rank`, which contains a 1-10 ranking of the most frequent words."}, {"cell_type": "code", "execution_count": 25, "metadata": {"scrolled": true}, "outputs": [], "source": "word_count_df = word_count_df.drop('Document Frequency', errors='ignore')"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": ["output_scroll"]}, "outputs": [], "source": "top_word_freq = get_top_n_counts(word_count_df)\ntop_word_freq"}, {"cell_type": "markdown", "metadata": {}, "source": "## Calculate tf\u2013idf"}, {"cell_type": "markdown", "metadata": {}, "source": "To calculate tf\u2013idf scores for every word, we're going to follow a very similar pattern with scikit-learn's [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n\nWhen you initialize TfidfVectorizer, you can choose to set it with different parameters. These parameters will change the way you calculate tf\u2013idf."}, {"cell_type": "markdown", "metadata": {}, "source": "### Without Smoothing or Normalization (Not Recommended)"}, {"cell_type": "markdown", "metadata": {}, "source": "Remember how we calculated the tf\u2013idf score for the word \"pigeons\" above?"}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"data": {"text/plain": ["88.3773044716594"]}, "execution_count": 27, "metadata": {}, "output_type": "execute_result"}], "source": "total_number_of_documents = 14 \nnumber_of_documents_with_term = 2\nterm_frequency = 30\ninverse_document_frequency = log(total_number_of_documents / number_of_documents_with_term) + 1\n\nterm_frequency * inverse_document_frequency"}, {"cell_type": "markdown", "metadata": {}, "source": "We can use this exact formula by running `TfidfVectorizer` and turning off smoothing (`smoth_idf=False`) and normalization (`norm=None`). This is **not** the best or recommended way to calculate tf\u2013idf scores. But it's useful to see the basic math that we discussed earlier in action with scikit-learn."}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": "#Initialize TfidfVectorizer with desired parameters (turn off smoothing and normalization)\ntfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english', smooth_idf = False, norm=None)\n\n#Plug in \"text_files\" which contains all our short stories\ntfidf_vector = tfidf_vectorizer.fit_transform(text_files)"}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [], "source": "#Make a DataFrame out of the tf\u2013idf vector and sort by title\ntfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\ntfidf_df = tfidf_df.sort_index()\n\n#Add column for number of times word appears in all documents\ntfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\ntfidf_slice"}, {"cell_type": "markdown", "metadata": {}, "source": "### With Smoothing and Normalization (Defaults/Recommended)"}, {"cell_type": "markdown", "metadata": {}, "source": "The recommended way to run `TfidfVectorizer`, however, is with smoothing (`smooth_idf = True`) and normalization (`norm='l2'`) turned on. These parameters will better account for differences in story length, and, overall, they'll produce more meaningful tf\u2013idf scores. \n\nSmoothing and L2 normalization are actually the default settings for `TfidfVectorizer`. To turn them on, you don't need to include any extra code at all."}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": "#Initialize TfidfVectorizer with desired parameters (default smoothing and normalization)\ntfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n\n#Plug in \"text_files\" which contains all our short stories\ntfidf_vector = tfidf_vectorizer.fit_transform(text_files)"}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": "#Make a DataFrame out of the tf\u2013idf vector and sort by title\ntfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\ntfidf_df = tfidf_df.sort_index()\n\n#Add column for number of times word appears in all documents\ntfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "tfidf_slice = tfidf_df[['pigeons', 'school', 'said', 'church', 'gospelteers', 'thunder','girl', 'street', 'father', 'dreaming', 'car']]\ntfidf_slice"}, {"cell_type": "markdown", "metadata": {}, "source": "To find out the top 10 words with the highest tf\u2013idf for every story, we're going to make and run the following function: `get_top_tfidf_scores()`"}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [], "source": "def get_top_tfidf_scores(series, top_n=10):\n    pretty_df = series.stack().groupby(level=0).nlargest(top_n).reset_index()\n    pretty_df = pretty_df.rename(columns={0:'tfidf_score', 'level_1': 'story', 'level_2': 'word'})\n    pretty_df = pretty_df.drop(columns='level_0')\n    pretty_df['tfidf_rank'] = pretty_df.groupby('story')['tfidf_score'].rank(method='first', ascending=False)\n    return pretty_df"}, {"cell_type": "markdown", "metadata": {}, "source": "As before, this function will rearrange the dataframe, `.groupby()` short story, and filter for the top 10 highest tf\u2013idf scores in every story. Finally, it will produce a dataframe with a new column `tfidf_rank`, which contains a 1-10 ranking of the highest tf\u2013idf scores."}, {"cell_type": "code", "execution_count": 35, "metadata": {"scrolled": true}, "outputs": [], "source": "tfidf_df = tfidf_df.drop('Document Frequency', errors='ignore')"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "top_tfidf = get_top_tfidf_scores(tfidf_df)\ntop_tfidf"}, {"cell_type": "markdown", "metadata": {}, "source": "## Write to a CSV File"}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": "filename = \"tfidf_Lost-in-The-City.csv\"\ntop_tfidf.to_csv(filename, encoding='UTF-8', index=False)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Compare Word Frequency and tf\u2013idf Scores"}, {"cell_type": "markdown", "metadata": {}, "source": "Now let's compare the raw word frequencies and tf-idf scores for all the stories in the *Lost in the City*."}, {"cell_type": "markdown", "metadata": {}, "source": "First, we're going to merge the top raw word frequency ranks into our top tf\u2013idf dataframe."}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [], "source": "tfidf_compare = top_tfidf.merge(top_word_freq[['word_freq_rank', 'word', 'story']] , on=['story', 'word'], how='left')"}, {"cell_type": "markdown", "metadata": {}, "source": "Then we're going to add a column that calculates the change in rank\u2014that is, how the significance of a word changes when we calculate tf-idf vs raw word frequency."}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [], "source": "tfidf_compare['changed_rank'] = tfidf_compare['word_freq_rank'] - tfidf_compare['tfidf_rank']\ntfidf_compare = tfidf_compare.fillna(\"*new top word*\")"}, {"cell_type": "markdown", "metadata": {}, "source": "Finally, we're going to make some functions that will alter the style of our Pandas dataframe\u2014such that the words that move up in tf-idf rank will be emphasized in green with a `+` sign and words that move down in tf-idf rank will be emphasized in red with a `-` sign."}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": "def make_positive(value):\n    if value != '*new top word*':\n        if float(value) > 0:\n            value = f'+{round(value)}'\n    return value\n\ndef make_bold(value):\n    return 'font-weight: bold'\n\ndef color_df(value):\n    if value == '*new top word*':\n        color = 'green'    \n    else:\n        value = str(value).replace('+', '')\n        value = float(value)\n        \n        if value < 0:\n            color = 'red'\n        elif value > 0:\n            color = 'green'\n        else:\n             color = 'black'        \n    df_style = f'color: {color}; font-weight: bold'\n    return df_style"}, {"cell_type": "markdown", "metadata": {}, "source": "Now let's display the dataframe and explore which words have become more significant and which words have become less so>"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "tfidf_compare['changed_rank'] = tfidf_compare['changed_rank'].apply(make_positive)\ntfidf_compare_styled = tfidf_compare.style.applymap(color_df, subset=['changed_rank']).applymap(make_bold, subset=['tfidf_rank'])\ntfidf_compare_styled"}, {"cell_type": "markdown", "metadata": {}, "source": "The word \"said,\" which is one of the most frequent words throughout the collection, gets knocked down in tf-idf importance precisely because it occurs in almost every story.\n\n*Note: To style your dataframe with color and bolding (as above), add `.style.applymap(color_df, subset=['changed_rank'])` to the end of the code below*"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "tfidf_compare[tfidf_compare['word'] == 'said']"}, {"cell_type": "markdown", "metadata": {}, "source": "A word like \"pigeons,\" on the other hand, becomes more significant because it is rarer."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "tfidf_compare[tfidf_compare['word'] == 'pigeons']"}, {"cell_type": "markdown", "metadata": {}, "source": "Words that were not frequent enough to make the top 10 for raw word frequency \u2014 such as \"dreaming,\" \"gospelteers,\" or \"dreadlocks \u2014 now suddenly show up in the top 10 for tf-idf scores."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "tfidf_compare[tfidf_compare['word'] == 'dreaming']"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "tfidf_compare[tfidf_compare['word'] == 'gospelteers']"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "tfidf_compare[tfidf_compare['word'] == 'dreadlocks']"}, {"cell_type": "markdown", "metadata": {}, "source": "## Your Turn!"}, {"cell_type": "markdown", "metadata": {}, "source": "Take a few minutes to explore the dataframe below and then answer the following questions."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "tfidf_compare"}, {"cell_type": "markdown", "metadata": {}, "source": "**1.** What is the difference between a tf-idf score and raw word frequency?"}, {"cell_type": "markdown", "metadata": {}, "source": "**#** Your answer here"}, {"cell_type": "markdown", "metadata": {}, "source": "**2.** Based on the dataframe above, what is one potential problem or limitation that you notice with tf-idf scores?"}, {"cell_type": "markdown", "metadata": {}, "source": "**#** Your answer here"}, {"cell_type": "markdown", "metadata": {}, "source": "**3.** What's another collection of texts that you think might be interesting to analyze with tf-idf scores?  Why?"}, {"cell_type": "markdown", "metadata": {}, "source": "**#** Your answer here"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.5"}}, "nbformat": 4, "nbformat_minor": 4}